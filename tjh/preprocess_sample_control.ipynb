{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from utils.tools import forward_fill_pipeline, normalize_dataframe, normalize_df_with_statistics, calculate_missing_rate, export_missing_mask_pipeline\n",
    "\n",
    "data_dir = \"./tjh/\"\n",
    "Path(os.path.join(data_dir, 'processed_sample')).mkdir(parents=True, exist_ok=True)\n",
    "Path(os.path.join(data_dir, 'statistics')).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_records = ['PatientID', 'RecordTime', 'AdmissionTime', 'DischargeTime']\n",
    "target_features = ['Outcome', 'LOS']\n",
    "demographic_features = ['Sex', 'Age']\n",
    "labtest_features = ['Hypersensitive cardiac troponinI', 'hemoglobin', 'Serum chloride', 'Prothrombin time', 'procalcitonin', 'eosinophils(%)', 'Interleukin 2 receptor', 'Alkaline phosphatase', 'albumin', 'basophil(%)', 'Interleukin 10', 'Total bilirubin', 'Platelet count', 'monocytes(%)', 'antithrombin', 'Interleukin 8', 'indirect bilirubin', 'Red blood cell distribution width ', 'neutrophils(%)', 'total protein', 'Quantification of Treponema pallidum antibodies', 'Prothrombin activity', 'HBsAg', 'mean corpuscular volume', 'hematocrit', 'White blood cell count', 'Tumor necrosis factorα', 'mean corpuscular hemoglobin concentration', 'fibrinogen', 'Interleukin 1β', 'Urea', 'lymphocyte count', 'PH value', 'Red blood cell count', 'Eosinophil count', 'Corrected calcium', 'Serum potassium', 'glucose', 'neutrophils count', 'Direct bilirubin', 'Mean platelet volume', 'ferritin', 'RBC distribution width SD', 'Thrombin time', '(%)lymphocyte', 'HCV antibody quantification', 'D-D dimer', 'Total cholesterol', 'aspartate aminotransferase', 'Uric acid', 'HCO3-', 'calcium', 'Amino-terminal brain natriuretic peptide precursor(NT-proBNP)', 'Lactate dehydrogenase', 'platelet large cell ratio ', 'Interleukin 6', 'Fibrin degradation products', 'monocytes count', 'PLT distribution width', 'globulin', 'γ-glutamyl transpeptidase', 'International standard ratio', 'basophil count(#)', 'mean corpuscular hemoglobin ', 'Activation of partial thromboplastin time', 'Hypersensitive c-reactive protein', 'HIV antibody quantification', 'serum sodium', 'thrombocytocrit', 'ESR', 'glutamic-pyruvic transaminase', 'eGFR', 'creatinine']\n",
    "require_impute_features = labtest_features\n",
    "normalize_features = ['Age'] + labtest_features + ['LOS']\n",
    "\n",
    "drop_columns = labtest_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_dir, \"processed_sample\", f\"tjh_dataset_formatted.csv\"))\n",
    "# df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified split dataset into `Training`, `Validation` and `Test` sets\n",
    "\n",
    "- Stratified dataset according to `Outcome` column\n",
    "- Baseline: 70% Training, 10% Validation, 20% Test (No Calibration)\n",
    "    - Name: train, val, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe by patient ID\n",
    "grouped = df.groupby('PatientID')\n",
    "\n",
    "# Get the patient IDs and outcomes\n",
    "patients = np.array(list(grouped.groups.keys()))\n",
    "patients_outcome = np.array([grouped.get_group(patient_id)['Outcome'].iloc[0] for patient_id in patients])\n",
    "\n",
    "# Get the train_val/test patient IDs\n",
    "train_val_patients, test_patients = train_test_split(patients, test_size=20/100, random_state=SEED, stratify=patients_outcome)\n",
    "\n",
    "# Get the train/val patient IDs\n",
    "train_val_patients_outcome = np.array([grouped.get_group(patient_id)['Outcome'].iloc[0] for patient_id in train_val_patients])\n",
    "train_patients, val_patients = train_test_split(train_val_patients, test_size=10/80, random_state=SEED, stratify=train_val_patients_outcome)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train, val, test, [traincal, calib] dataframes for the current fold\n",
    "train_df = df[df['PatientID'].isin(train_patients)]\n",
    "val_df = df[df['PatientID'].isin(val_patients)]\n",
    "test_df = df[df['PatientID'].isin(test_patients)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(data_dir, 'processed_sample', 'fold_0') # forward fill\n",
    "Path(save_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and std of the train set (include age, lab test features, and LOS) on the data in 5% to 95% quantile range\n",
    "train_df, val_df, test_df, default_fill, los_info, train_mean, train_std = normalize_dataframe(train_df, val_df, test_df, normalize_features)\n",
    "\n",
    "train_x, train_y, train_pid = forward_fill_pipeline(train_df, default_fill, demographic_features, labtest_features, target_features, require_impute_features)\n",
    "val_x, val_y, val_pid = forward_fill_pipeline(val_df, default_fill, demographic_features, labtest_features, target_features, require_impute_features)\n",
    "test_x, test_y, test_pid = forward_fill_pipeline(test_df, default_fill, demographic_features, labtest_features, target_features, require_impute_features)\n",
    "\n",
    "# Save the imputed dataset to pickle file\n",
    "pd.to_pickle(train_x, os.path.join(save_dir, \"train_x.pkl\"))\n",
    "pd.to_pickle(train_y, os.path.join(save_dir, \"train_y.pkl\"))\n",
    "pd.to_pickle(train_pid, os.path.join(save_dir, \"train_pid.pkl\"))\n",
    "pd.to_pickle(val_x, os.path.join(save_dir, \"val_x.pkl\"))\n",
    "pd.to_pickle(val_y, os.path.join(save_dir, \"val_y.pkl\"))\n",
    "pd.to_pickle(val_pid, os.path.join(save_dir, \"val_pid.pkl\"))\n",
    "pd.to_pickle(test_x, os.path.join(save_dir, \"test_x.pkl\"))\n",
    "pd.to_pickle(test_y, os.path.join(save_dir, \"test_y.pkl\"))\n",
    "pd.to_pickle(test_pid, os.path.join(save_dir, \"test_pid.pkl\"))\n",
    "pd.to_pickle(los_info, os.path.join(save_dir, \"los_info.pkl\")) # LOS statistics (calculated from the train set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for fold_1 to fold_9, reduce sample size from 10% to 90%\n",
    "\"\"\"\n",
    "save_dir = os.path.join(data_dir, 'processed_sample', 'fold_0') # forward fill\n",
    "# Load the data\n",
    "train_x = pd.read_pickle(os.path.join(save_dir, \"train_x.pkl\"))\n",
    "train_y = pd.read_pickle(os.path.join(save_dir, \"train_y.pkl\"))\n",
    "\n",
    "train_x_ = []\n",
    "train_y_ = []\n",
    "for i in range(len(train_x)):\n",
    "    for j in range(len(train_x[i])):\n",
    "        cur_x = train_x[i][j]\n",
    "        cur_y = train_y[i][j]\n",
    "        train_x_.append((i, cur_x))\n",
    "        train_y_.append((i, cur_y))\n",
    "\n",
    "for fold in range(1, 10):\n",
    "\n",
    "    print(\"Fold: {}\".format(fold))\n",
    "\n",
    "    # Set the percentage to be dropped\n",
    "    drop_percentage = 0.1*fold\n",
    "\n",
    "    np.random.seed(42)\n",
    "    # Randomly select the indices to be dropped\n",
    "    drop_indices = np.random.choice(np.arange(len(train_x_)), int(len(train_x_) * drop_percentage))\n",
    "\n",
    "    # Drop the selected indices from the lists\n",
    "    fold_train_x_ = [train_x_[i] for i in range(len(train_x_)) if i not in drop_indices]\n",
    "    fold_train_y_ = [train_y_[i] for i in range(len(train_y_)) if i not in drop_indices]    \n",
    "\n",
    "    pid_ = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "    for i in range(len(fold_train_x_)):\n",
    "        cur_pid = fold_train_x_[i][0]\n",
    "        cur_x = fold_train_x_[i][1]\n",
    "        cur_y = fold_train_y_[i][1]\n",
    "\n",
    "        if cur_pid not in pid_:\n",
    "            pid_.append(cur_pid)\n",
    "            x_.append([cur_x])\n",
    "            y_.append([cur_y])\n",
    "        else:\n",
    "            x_[pid_.index(cur_pid)].append(cur_x)\n",
    "            y_[pid_.index(cur_pid)].append(cur_y)\n",
    "\n",
    "    save_dir = os.path.join(data_dir, 'processed_sample', f'fold_{fold}') # forward fill\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pd.to_pickle(x_, os.path.join(save_dir, \"train_x.pkl\"))\n",
    "    pd.to_pickle(y_, os.path.join(save_dir, \"train_y.pkl\"))\n",
    "    pd.to_pickle(pid_, os.path.join(save_dir, \"train_pid.pkl\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
